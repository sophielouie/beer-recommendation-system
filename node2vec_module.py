# -*- coding: utf-8 -*-
"""Node2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qaoTfIxbNlaqne5y8wtIzA-ySkCMbzGA
"""


import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from scipy import spatial
import statistics as stats
import math
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import networkx as nx
import node2vec
from node2vec import Node2Vec as n2v

THRESHOLD = 0.875

"""# Data Collection"""

def clean_beer_reviews():
    # storing beer review dataset
    beer_reviews = pd.read_csv("beer_reviews.csv")
    # creating a unique identifier for each beer using brewery name and beer name
    beer_reviews['Unique Beer Name'] = beer_reviews['brewery_name'] + ' ' + beer_reviews['beer_name']
    # storing beer profile dataset
    beer_profile = pd.read_csv("beer_profile_and_ratings.csv", error_bad_lines=False)
    # columns to drop from beer reviews
    drop_cols = ['brewery_id', 'brewery_name',  'beer_name', 'beer_abv', 'beer_beerid']
    # dropping columns from beer reviews
    beer_reviews.drop(columns = drop_cols, inplace = True)
    # columns to drop from  beer profile
    drop_cols = ['Name', 'Style', 'Brewery', 'Description',
       'Min IBU', 'Max IBU', 'Alcohol', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste',
       'review_overall', 'number_of_reviews']
    # dropping columns from beer profile
    beer_profile.drop(columns = drop_cols, inplace = True)
    # combining beer review and beer profile datasets to have profile of each beer attached to every review
    df_beer = pd.merge(beer_reviews, beer_profile, left_on = 'Unique Beer Name', right_on = 'Beer Name (Full)', how = 'inner')
    # isolating the numerical columns that need to be scaled
    need_scaling = df_beer.drop(columns = ['review_time', 'review_profilename', 'beer_style', 'Unique Beer Name', 'Beer Name (Full)'])
    # storing the informational portion of the dataset that does not need scaling
    informational = df_beer[['review_time', 'review_profilename', 'beer_style', 'Unique Beer Name', 'Beer Name (Full)']]
    # renaming beer name column
    informational.rename(columns = {'Beer Name (Full)': 'Beer Name'}, inplace = True)

    # scaling the data
    scaler = MinMaxScaler()
    scaler.fit(need_scaling)
    need_scaling = pd.DataFrame(scaler.transform(need_scaling), columns = need_scaling.columns)

    # recombining the informational data and scaled data
    df = pd.concat([informational, need_scaling], axis = 1)
    return df

"""#Defining Parameters before Testing

We need to select users to isolate for our test set. The criteria for these users is that they should have reviewed enough beers that there is some likelihood that they would have tried something that has been recommended to them. Starting with binary cumulative gain, we will assess the performance of our recommendation system by determining if the recommended beers have been tried by the user or exceeded a threshold.

- recommend x beers to 10 users
- traverse the recommended beer list, see how many have been rated highly
- compare the accuracy ratings of the three test users
- determine threshold for positive review by finding percentiles of ratings (top 1/4 review would be considered a successful recommendation)

## Choosing users for our test set
"""





"""Only 219,603 / 744,251 reviews (29.5%) were given a score of .875 or greater, so this will be our threshold for a successful recommendation

## Remove all ratings by the test users from the training set except for the beer they rated the highest
"""

def create_train_test_split(frac_rem=1):
    #frac_rem: fraction of each user in test set to remain in train set
    # train_set is copy of df --> df has review_profilename and Unique Beer Name as indicies
    # we are preserving user information by reseting indicies, so when we .loc[] we still have access to user and beer info
    train_set = df.copy().reset_index()
    test_parameters = []
    test_set = pd.DataFrame(columns = train_set.columns)


    for user in test_users: 
        # all reviews for a the test user
        user_reviews = train_set.loc[train_set.review_profilename == user]

        # sorted reviews
        user_reviews = user_reviews.sort_values(by = 'review_overall', ascending = False)

        # store highest reviewed beer
        highest_reviewed_beer = user_reviews.iloc[0]
        test_parameters.append((user, highest_reviewed_beer["Beer Name"]))

        # calculating the last index to remove from the train set
        last_idx = int((len(user_reviews) - 1) * frac_rem)

        # concatenate the removed user-beer pairs to test set
        test_set = pd.concat([test_set, user_reviews.iloc[1:last_idx]])

        # remove all beers from training set, add back in highest_reviewed_beer
        train_set.drop(user_reviews.iloc[1:last_idx].index, axis = 0, inplace = True)

    return train_set, test_set, test_parameters

def calc_score(metric, rec_dict, n_recs, test):
  # test is a dataframe consisting of the user reviews to be used to determine relevancy
  score = 0
  index = 1
  num_rel = 0
  for user, recs in rec_dict.items():

    for rec in recs:

      # we need to include Raise And Catch Exception when beer is not in test set
      row = test.loc[(test.review_profilename == user) & (test["Unique Beer Name"] == rec)]

      # if rec in test_set and above threshold
      if len(row) > 0:
        if row["review_overall"].iloc[0] >= THRESHOLD:
          num_rel += 1

          if metric == "CG":
            score += 1

          elif metric in ["DCG", 'NDCG']:
            score += (1 / math.log2(index + 1))

          elif metric == "MAP":
            score += num_rel / index

          else:
            raise Exception("Metric type provided is not valid.")

    if metric == 'NDCG':
      ideal = 0
      for num in range(num_rel):
        # num starts at 0, so we add two to mimic the starting index of 1
        ideal += (1 / math.log2(num + 2))
      score = score / ideal

    if metric == "MAP":
      score = score / n_recs

    index += 1
    print(f"{user} : {score}")

"""# Node2Vec"""



#iterate through each column; in each column we want to qcut on that column by x bins
#Make a new column that describes which buckets the beer falls into for each descriptor
#We should have a DF where each beer has a column describing the profile buckets it is in 
#These will be used make links between beers
def bucket_me(x, df):
  for col in df.columns:
    if col != 'Style':
      #Not passing labels in this step to ensure # of labels < bin edges
      categories, bins = pd.qcut(df[col], q=x, duplicates = 'drop', retbins = True)
      labels = []
      for i in range(len(bins) - 1):
        labels.append(col + str(i))
      df[col] = pd.qcut(df[col], q=x,labels=labels,duplicates='drop')
  return df

def str_to_list(input):
  return input.split(',')

"""###Node Graph

This function will generate a article to article network given an input DataFrame. It will do so by creating an edge_dictionary where each key is going to be a node referenced by unique values in node_col and the values will be a list of other nodes connected to the key through the edge_col.
"""

def generate_network(df, edge_col = "buckets", shared_att = 3):
    edge_dct = {}

    # iterating for each unique beer in the df
    for beer in list(df.index):
        # get  "all topic" of the beer
        beer_topics = df.loc[beer][edge_col]

        # creating a list of all the beers that are not the current one and share X attributes
        edge_df = df[(df.index != beer) & (df[edge_col].apply(beer_comparer, args = (beer_topics, shared_att, )))]
        edge_dct[beer] = edge_df.index
    
    # create nx network
    g = nx.Graph(edge_dct, create_using = nx.MultiGraph)
    return g

def beer_comparer(comp_list, beer_list, shared_att):
  # compares two lists and returns True if number of shared values is greater than or equal to shared_att
  return len([i for i, j in zip(comp_list, beer_list) if i == j]) >= shared_att


def predict_links(g, df, beer_name, num_rec):
    #dataframe with just row of given beer name
    this_beer = df[df.index == beer_name]

    #getting beers which are not already linked to the given beer
    all_nodes = g.nodes()
    #list of all beer names that are not adjacent to the beer
    all_other_nodes = [n for n in all_nodes if n not in list(g.adj[beer_name]) + [beer_name]]
    #DataFrame that contains non-adjacent nodes
    other_nodes = df[df.index.isin(all_other_nodes)]
    #find the cosine similarity between the given beer and all beers that are not already neighbors
    similar = dict()
    for beer in other_nodes.iterrows():
        similar[beer[0]] = (1 - spatial.distance.cosine(beer[1], np.array(this_beer)))
    #sort the dictionary by highest cosine similarity
    similar = pd.DataFrame(similar.items(), columns = ['beer', 'cos sim'])
    sorted_sim = similar.sort_values(by = 'cos sim', ascending = False)
    return sorted_sim['beer'].iloc[0:num_rec]

def node2vec(beer, n_recs):
    import pickle
    
    
    # Loading in pkl file storing graph
    filehandler = open('beer_network.pkl', 'rb') 
    g = pickle.load(filehandler)
    # Reading in the embedding dataframe
    emb_df = pd.read_csv('embedding_df.csv')
    # Set embedding index to beer names
    emb_df.set_index(['Unnamed: 0'], inplace=True)
    
    return predict_links(g, emb_df, beer, n_recs)

# node2vec(4, 7, 'Alaskan Brewing Co. Alaskan Amber', 10)

"""calcscore, df, test_parameters

## Testing Node2Vec
- Iterate through test parameters (most frequent reviewers and their top reviewed beers), recommend them each ten beers, and see how they rated those beers
"""

# Using the beer_profile dataframe, we want to compare 
def compile_rec_list_n2v(n_recs):
  rec_dict = {}
  # iterate through test users, get X recommendations for their only beer review, compare recommendations to actual ratings stored in test_set using DCG
  for user, beer in test_parameters[0:5]:
    recommendations = predict_links(g, emb_df, beer, n_recs)
    rec_dict[user] = list(recommendations)

  return rec_dict